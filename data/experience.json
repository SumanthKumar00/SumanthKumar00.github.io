{
  "_instructions": "Add your work experience here. Copy the experience object for each job.",
  "sectionTitle": "Professional Experience",
  "experiences": [
    {
      "title": "Data Engineer",
      "company": "WellStar Health System | GA",
      "period": "Aug 2024 - Current",
      "location": "",
      "description": "Building Python/SQL ETL workflows and scalable data pipelines using PySpark and Azure Data Factory for high-volume healthcare data processing. Designing data warehouse models in Snowflake and Azure Synapse, optimizing Power BI dashboards (30% faster), and implementing data quality validation in Databricks (15% fewer inconsistencies).",
      "responsibilities": [
        "Built Python/SQL ETL workflows processing high-volume EHR, claims, and patient data with quality checks and partitioning strategies",
        "Developed scalable data pipelines using PySpark and Azure Data Factory, automating ingestion from EHR systems and clinical applications",
        "Designed data warehouse models in Snowflake and Azure Synapse using Star schemas for clinical and financial reporting",
        "Improved Power BI dashboard performance by 30% and reduced data inconsistencies by 15% via Databricks validation logic"
      ]
    },
    {
      "title": "Data Engineer",
      "company": "Cognizant | India",
      "period": "Dec 2021 - Aug 2023",
      "location": "",
      "description": "Built scalable data pipelines using PySpark and AWS Glue for payment gateways and banking systems, reducing manual intervention by 35%. Developed Tableau dashboards for compliance metrics, implemented data quality validations (20% fewer errors), and processed high-volume transactions using Hadoop HDFS.",
      "responsibilities": [
        "Built scalable data pipelines using PySpark and AWS Glue for payment gateways and banking systems, reducing manual intervention by 35%",
        "Developed Tableau dashboards for payment authorizations, fraud alerts, and compliance metrics with optimized load times",
        "Implemented data quality validations and partitioning strategies, reducing processing errors by 20%",
        "Designed MongoDB collections for semi-structured transaction data and audit logs with flexible schema evolution",
        "Processed high-volume banking transactions using Hadoop HDFS; monitored Apache NiFi pipelines for bottleneck prevention"
      ]
    }
  ]
}